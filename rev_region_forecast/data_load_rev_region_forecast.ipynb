{"cells":[{"cell_type":"code","source":["\n","# define widgets - NEED TO DEFINE IT ONCE\n","# dynamic variables (pass it from ADF)\n","# first time runtime parameter\n","# dbutils.widgets.dropdown(\"environment\", \"dev\", [\"dev\",\"uat\",\"prod\"])\n","# dbutils.widgets.dropdown(\"new_training\", \"False\", [\"True\",\"False\"])\n","# dbutils.widgets.dropdown(\"system_name\", \"bimodelapi\", [\"bimodelapi\"])\n","# dbutils.widgets.text(\"system_name\", \"\",\"\")\n","# dbutils.widgets.remove(\"pbiapi\")\n","environment = dbutils.widgets.get(\"environment\")\n","new_training = dbutils.widgets.get(\"new_training\")\n","system_name = dbutils.widgets.get(\"system_name\")\n","if environment in {\"prod\"}:\n","    aml_compute_cluster_name = \"cc-bi-ml-prod01\"\n","else:\n","    aml_compute_cluster_name = \"cc-bi-ml-devqa01\"\n","\n","print(environment, system_name, new_training, aml_compute_cluster_name)\n","\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9489c9e1-c3c0-4f7a-9da8-9d32c1ee9219"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["%run /bi_config/lib_helper"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e69ea4b-e667-40fa-9f87-348bc16990a9"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["%run /bi_config/lib_sentry"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b1233b9-fbb7-4cf4-b245-f8386d4f5422"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["%run ./helper"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a0e5a6b5-2a13-40ae-af87-70a9fa0ac07f"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["\n","# ============================= pyspark libraries import ===================================#\n","from pyspark.sql.functions import (\n","    col,\n","    concat,\n","    lit,\n","    current_date,\n","    lag,\n","    lead,\n","    first,\n","    last,\n","    desc,\n","    hash,\n","    row_number,\n",")\n","from datetime import date, timedelta, datetime\n","from pyspark.sql import functions as F\n","from pyspark.sql import *\n","from delta.tables import *\n","from pyspark.sql.types import TimestampType, LongType, StringType\n","\n","# from ibi_packages import functions as fn\n","import os\n","import stringcase\n","from pyspark.sql import SQLContext\n","from pyspark.sql.window import Window\n","\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20682479-7339-4b5d-9d83-105195fbbaf9"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["\n","# ================= Load BI config parameters data (overwrite) =================#W\n","# static variables\n","environment_name = environment\n","filename = \"automlrevfoecastconfig.json\"\n","\n","today = pd.to_datetime(\"today\").normalize()\n","current_eom = today + pd.offsets.MonthEnd(0)\n","start_date = \"2017-01-01\"\n","end_date = current_eom + pd.offsets.MonthEnd(11)\n","\n","notebook = os.path.basename(getNotebookPath())\n","\n","input_data_path = \"/dbfs/mnt/\" + environment + \"/automl_rev_region_forecast/inputs/\"\n","output_data_path = \"/dbfs/mnt/\" + environment + \"/automl_rev_region_forecast/outputs/\"\n","blobstore_datadir = \"revregionforecast_data/\"\n","\n","bi_config_parameter_filepath = \"/mnt/{}/automl_rev_region_forecast/config/{}\".format(\n","    environment_name, filename\n",")\n","\n","try:\n","    # read JSON file\n","    df_bi_config_parameters = (\n","        spark.read.format(\"json\")\n","        .option(\"multiline\", \"true\")\n","        .load(bi_config_parameter_filepath)\n","    )\n","    df_bi_config_parameters = df_bi_config_parameters.filter(\n","        df_bi_config_parameters.SystemName == lit(system_name)\n","    )\n","    display(df_bi_config_parameters)\n","\n","    # adding audit fields\n","    # df_bi_config_parameters = df_bi_config_parameters.withColumn(\"IBICreatedBy\",lit(ibi_created_by))\n","    # df_bi_config_parameters = df_bi_config_parameters.withColumn(\"IBIUpdatedBy\",lit(ibi_updated_by))\n","    # df_bi_config_parameters = df_bi_config_parameters.withColumn(\"IBICreatedDate\",lit(ibi_created_date).cast(TimestampType()))\n","    # df_bi_config_parameters = df_bi_config_parameters.withColumn(\"IBIUpdatedDate\",lit(ibi_updated_date).cast(TimestampType()))\n","    # df_bi_config_parameters = df_bi_config_parameters.withColumn('ID', row_number().over(Window.orderBy('EnvironmentName','SystemName')))\n","\n","    # initializing config parameter values\n","\n","    # subscription_id = \"db61fd47-db56-45e3-844f-1b1f5c47990a\" #you should be owner or contributor\n","    subscription_id = dbutils.secrets.get(\n","        scope=\"kv-bi-devqa-01-secrets\", key=\"subscription-id\"\n","    )\n","\n","    resource_group = (\n","        df_bi_config_parameters.filter(\n","            df_bi_config_parameters.ParameterName == \"ws_resource_group\"\n","        )\n","        .select(\"ParameterValue\")\n","        .collect()[0][0]\n","    )\n","    workspace_name = (\n","        df_bi_config_parameters.filter(\n","            df_bi_config_parameters.ParameterName == \"workspace_name\"\n","        )\n","        .select(\"ParameterValue\")\n","        .collect()[0][0]\n","    )\n","    workspace_region = (\n","        df_bi_config_parameters.filter(\n","            df_bi_config_parameters.ParameterName == \"workspace_region\"\n","        )\n","        .select(\"ParameterValue\")\n","        .collect()[0][0]\n","    )\n","\n","\n","except Exception as error:\n","    log_error(\"{} {}\".format(notebook, error))\n","\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"178f4767-b219-4821-a874-33c6c5f6ecb4"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["\n","# download adjusted closing prices from Yahoo finance\n","#!pip install pandas_datareader\n","#!pip install yfinance\n","import pandas as pd\n","import yfinance as yf\n","import datetime\n","import time\n","import requests\n","import io\n","\n","pd.set_option(\"display.float_format\", lambda x: \"%.2f\" % x)\n","\n","start = datetime.datetime(2016, 1, 1)\n","end = today\n","\n","# Define the instruments to download. We would like to see Apple, Microsoft and the S&P500 index.\n","symbols = [\n","    \"^GSPC\",\n","    \"^FTSE\",\n","    \"^N100\",\n","    \"^N225\",\n","    \"000001.SS\",\n","]\n","symbols_name = [\n","    \"SP_500\",\n","    \"FTSE_100\",\n","    \"EURONEXT_100\",\n","    \"Nikkei_225\",\n","    \"SSE_Composite_Index\",\n","]\n","\n","\n","# create empty dataframe\n","stock_final = pd.DataFrame()\n","# iterate over each symbol\n","for i in symbols:\n","\n","    # print the symbol which is being downloaded\n","    print(str(symbols.index(i)) + str(\" : \") + i, sep=\",\", end=\",\", flush=True)\n","\n","    try:\n","        # download the stock price\n","        stock = []\n","        stock = yf.download(i, start=start, end=end, progress=False)\n","\n","        # append the individual stock prices\n","        if len(stock) == 0:\n","            None\n","        else:\n","            stock[\"Name\"] = i\n","            stock_final = stock_final.append(stock, sort=False)\n","    except Exception as error:\n","        print(error)\n","        log_error(\"{} {}\".format(notebook, error)) #log error in sentry\n","        raise dbutils.notebook.exit(error) #raise the exception\n","\n","# display(stock_final.tail())\n","\n","stock_final1 = stock_final.copy()\n","stock_final1 = stock_final1.reset_index(level=stock_final1.index.names)\n","\n","stock_final1[\"Name\"].where(stock_final1[\"Name\"] != \"^N225\", \"Nikkei_225\", inplace=True)\n","stock_final1[\"Name\"].where(\n","    stock_final1[\"Name\"] != \"000001.SS\", \"SSE_Composite_Index\", inplace=True\n",")\n","stock_final1[\"Name\"].where(stock_final1[\"Name\"] != \"^GSPC\", \"SP_500\", inplace=True)\n","stock_final1[\"Name\"].where(stock_final1[\"Name\"] != \"^FTSE\", \"FTSE_100\", inplace=True)\n","stock_final1[\"Name\"].where(\n","    stock_final1[\"Name\"] != \"^N100\", \"EURONEXT_100\", inplace=True\n",")\n","\n","stock_final1[\"End_of_Month\"] = stock_final1[\"Date\"] + pd.offsets.MonthEnd(0)\n","\n","\n","# display(stock_final1.tail())\n","stock_final1 = stock_final1.sort_values(\"Date\").groupby([\"End_of_Month\", \"Name\"]).last()\n","stock_final1 = stock_final1.drop(\n","    [\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"], axis=1\n",")\n","stock_final1 = stock_final1.reset_index(level=stock_final1.index.names)\n","\n","# Drop all Not a number values using drop method.\n","stock_final1.dropna(inplace=True)\n","stock_final_pivot = stock_final1.pivot_table(\n","    index=\"End_of_Month\", values=\"Adj Close\", margins=False, columns=[\"Name\"]\n",")\n","# aggfunc=['sum','count'],\n","\n","# revhist_pivot = revhist_pivot.rename(columns={'sum':'Revenue'})\n","\n","stock_final_pivot = stock_final_pivot.reset_index(\n","    level=stock_final_pivot.index.names\n",").reset_index(drop=True)\n","\n","# revhist_pivot['End_of_Month'] = pd.to_datetime(revhist_pivot['End_of_Month'])#.dt.date\n","# revhist_pivot.index.name = 'Date'\n","\n","stock_final_pivot.fillna(0, inplace=True)\n","# stock_final1.query('Name==\"Nikkei 225\"').plot(title=str(i))\n","#display(stock_final_pivot.info())\n","#display(stock_final_pivot.tail())\n","for i in symbols_name:\n","    stock_final_pivot[i].plot(legend=True)\n","\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42f54a77-712c-48d4-9ed6-676b3950e8fc"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["\n","# Load Power BI Data into pandas Dataframe\n","# https://stbidatadevqa01.dfs.core.windows.net/dev/automl_rev_region_forecast\n","\n","# Ingest power bi data\n","\n","import glob\n","\n","init_time_column_name = \"End of Month\"\n","init_time_column_name1 = \"Snapshot Date Short\"\n","init_time_column_name2 = \"Snapshot End of Month\"\n","try:\n","    talenthist = pd.read_csv(\n","        input_data_path + \"TalentHistory.csv\", parse_dates=[init_time_column_name]\n","    )\n","    revhist = pd.read_csv(\n","        input_data_path + \"RevenueHistory.csv\", parse_dates=[init_time_column_name]\n","    )\n","    opphist = pd.read_csv(\n","        input_data_path + \"OppHistory.csv\", parse_dates=[init_time_column_name]\n","    )\n","    projhist = pd.read_csv(\n","        input_data_path + \"ProjectHistory.csv\", parse_dates=[init_time_column_name]\n","    )\n","    pipetrend = pd.read_csv(\n","        input_data_path + \"PipelineTrend.csv\",\n","        parse_dates=[init_time_column_name1, init_time_column_name2],\n","    )\n","    projectactualshist = pd.read_csv(\n","        input_data_path + \"ProjectActualsHistory.csv\",\n","        parse_dates=[init_time_column_name],\n","    )\n","\n","    # pipehist = pd.read_csv(input_data_path + 'PipelineHistory*.csv',\n","    #                       parse_dates=[init_time_column_name,\n","    #                       init_time_column_name1])\n","\n","    pipehist = pd.concat(\n","        [\n","            pd.read_csv(f, parse_dates=[init_time_column_name, init_time_column_name1])\n","            for f in glob.glob(input_data_path + \"PipelineHistory*.csv\")\n","        ],\n","        ignore_index=True,\n","    )\n","\n","    # Remove Percentage from numbers if existinput_data_path + 'PipelineHistory*.csv'\n","\n","    # opphist1.filter(like=\"Win_Rate\", axis=1).columns\n","\n","    cols_float1 = [\"Win Rate\"]\n","    remove_percetage(opphist, cols_float1)\n","    cols_float2 = [\"Yield\"]\n","    remove_percetage(pipehist, cols_float2)\n","    cols_float3 = [\"Utilization Billable\"]\n","    remove_percetage(projectactualshist, cols_float3)\n","    \n","    file_list = [\n","        pipehist,\n","        talenthist,\n","        revhist,\n","        opphist,\n","        projhist,\n","        pipetrend,\n","        projectactualshist\n","    ]\n","\n","    for x in file_list:\n","\n","        # replace blank with underscore\n","\n","        x.columns = x.columns.astype(str).str.replace(\" \", \"_\")\n","\n","        # remove contrib or managing\n","\n","        x.columns = x.columns.astype(str).str.replace(\"Contrib_\", \"\")\n","        x.columns = x.columns.astype(str).str.replace(\"Managing_\", \"\")\n","        '''\n","        print(get_df_name(x), \":\", x.shape)\n","        display(x.info())\n","        display(x.tail())\n","        '''\n","except Exception as error:\n","    print(error)\n","    log_error(\"{} {}\".format(notebook, error)) #log error in sentry\n","    raise dbutils.notebook.exit(error) #raise the exception\n","\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b197df27-46c5-46fa-aa67-6e0f4ef5ebb6"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["pipehist[\"Snapshot_Date_Short\"].unique()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4e1a534-565c-4185-a2cd-15ceb25f399e"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["pipehist[\"End_of_Month\"].unique()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62b2ca00-fe23-4d74-8f59-c85d3da5f8dc"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":[],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8415477a-4386-4259-ade7-80fe9b433be3"}},"outputs":[],"execution_count":null}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"data_load_rev_region_forecast","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{"new_training":{"nuid":"0f348729-f026-4d4a-bb66-401234fe1cbe","currentValue":"False","widgetInfo":{"widgetType":"dropdown","name":"new_training","defaultValue":"False","label":null,"options":{"widgetType":"dropdown","choices":["True","False"]}}},"system_name":{"nuid":"5fd2a812-8996-4edf-8b3e-231a4a96f596","currentValue":"bimodelapi","widgetInfo":{"widgetType":"dropdown","name":"system_name","defaultValue":"bimodelapi","label":null,"options":{"widgetType":"dropdown","choices":["bimodelapi"]}}},"environment":{"nuid":"a96f3b71-a246-4b68-8446-4fa02bb559ba","currentValue":"dev","widgetInfo":{"widgetType":"dropdown","name":"environment","defaultValue":"dev","label":null,"options":{"widgetType":"dropdown","choices":["dev","uat","prod"]}}}},"notebookOrigID":1689033015852645}},"nbformat":4,"nbformat_minor":0}