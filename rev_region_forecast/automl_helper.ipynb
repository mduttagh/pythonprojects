{"cells":[{"cell_type":"code","source":["#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n# Definitions\n\nimport pandas as pd\nimport numpy as np\n\ntoday = pd.to_datetime('today').normalize()\ncurrent_eom = today + pd.offsets.MonthEnd(0)\nstart_date = '2017-01-01'\nend_date = current_eom + pd.offsets.MonthEnd(11)\n\n# entity_debug = \"GBR\"\n\ndebug = False\n\n# display count and summary of any dataframe\n\npd.set_option('display.max_columns', None)\npd.set_option('display.expand_frame_repr', False)\n\n# pd.set_option('max_colwidth', -1)\n\npd.set_option('display.precision', 1)\npd.set_option('display.float_format', lambda x: '%.2f' % x)\n\n\ndef get_json(df):\n    \"\"\" Small function to serialise DataFrame dates as 'YYYY-MM-DD' in JSON \"\"\"\n\n    def convert_timestamp(item_date_object):\n        if isinstance(item_date_object, (datetime.date,\n                      datetime.datetime)):\n            return item_date_object.strftime('%Y-%m-%d')\n\n    dict_ = df.to_dict(orient='records')\n\n    return json.dumps(dict_, default=convert_timestamp)\n\n\ndef get_df_name(df):\n    name = [x for x in globals() if globals()[x] is df][0]\n    return name\n\n\ndef difflist(li1, li2):\n    return list(set(li1) - set(li2))\n\n\ndef addlist(li1, li2):\n    return li1.append(li2)\n\n\ndef remove_percetage(df, column_list):\n    for col in column_list:\n        df[col] = round(df[col].str.replace('%', '').astype(np.float64)\n                        / 100, 4)\n    return df\n\nnumeric_cols = [\"\"]\ndef coerce_to_numeric(df, num_cols = numeric_cols):\n    for col in num_cols:\n        if col in df.columns:\n            df[col] = (\n                (df[col]\n                .replace( '[\\$,)]','', regex=True)\n                .replace( '[(]','-',   regex=True ).astype(float)) #.apply(pd.to_numeric, errors=\"coerce\")\n                .replace(np.nan, 0, regex=True)\n            )\n    return df\n  \nint_cols = [\"\"]\ndef coerce_to_int(df, int_cols = int_cols):\n    for col in int_cols:\n        if col in df.columns:\n            df[col] = (\n                df[col]\n                .apply(pd.to_numeric, errors=\"coerce\")\n                .replace(np.nan, 0, regex=True)\n                .astype(int)\n            )\n    return df\n\ndef convert_date_cols(df):\n    if 'End_of_Month' in df.columns:\n        df['End_of_Month'] = pd.to_datetime(df['End_of_Month'])\n    if 'Snapshot_Date_Short' in df.columns:\n        df['Snapshot_Date_Short'] = \\\n            pd.to_datetime(df['Snapshot_Date_Short'])\n    if 'Snapshot_Date' in df.columns:\n        df['Snapshot_Date'] = pd.to_datetime(df['Snapshot_Date'])\n    if 'Forecast_Date' in df.columns:\n        df['Forecast_Date'] = pd.to_datetime(df['Forecast_Date'])\n    return df\n\n\ndef data_prep(df):\n    df.columns = df.columns.astype(str).str.replace(' ', '_')\n    df = convert_date_cols(df)\n    df = df.replace(np.nan, 0, regex=True)\n    return df\n  \n\ndef move_column_inplace(df, col, pos):\n    col = df.pop(col)\n    df.insert(pos, col.name, col)\n\n\ndef show_stats(df):\n    display(' DF Name: ')\n    display(get_df_name(df))\n    display(' DF Info: ')\n    display(df.info(verbose=True))\n    display(' DF Describe: ')\n    display(df.describe(include='all')) #.transpose().head()\n    display(' DF Head: ')\n    display(df.head())\n    display(' DF Tail: ')\n    display(df.tail())\n\n    # group_by_entity = df.groupby(by=['Fin_Entity_ID'], as_index=False)\n    # entity_sum = group_by_entity.sum().reset_index(drop=True)\n    # entity_count = group_by_entity.count().reset_index(drop=True)\n    # print(\" Entity Sum: \")\n    # display(entity_sum.head())\n    # print(\" Studio Count: \")\n    # display(entity_count.head())\n\n    if 'End_of_Month' in df.columns:\n        df['End_of_Month'] = pd.to_datetime(df['End_of_Month'])  # Format Date\n        group_by_eom = df.groupby(by=['End_of_Month'], as_index=False)\n        eom_sum = group_by_eom.sum().reset_index(drop=True)\n        eom_count = group_by_eom.count().reset_index(drop=True)\n        display(' EOM Sum:')\n        display(eom_sum.head())\n        display(' EOM Count: ')\n        display(eom_count.head())\n    if 'Snapshot_Date_Short' in df.columns:\n        df['Snapshot_Date_Short'] = \\\n            pd.to_datetime(df['Snapshot_Date_Short'])  # Format Date\n        group_by_sds = df.groupby(by=['Snapshot_Date_Short'],\n                                  as_index=False)\n        sds_sum = group_by_sds.sum().reset_index(drop=True)\n        sds_count = group_by_sds.count().reset_index(drop=True)\n        display(' SDS Sum:')\n        display(sds_sum.head())\n        display(' SDS Count: ')\n        display(sds_count.head())\n    if 'Snapshot_Date' in df.columns:\n        df['Snapshot_Date'] = pd.to_datetime(df['Snapshot_Date'])  # Format Date\n        group_by_sds = df.groupby(by=['Snapshot_Date'], as_index=False)\n        sds_sum = group_by_sds.sum().reset_index(drop=True)\n        sds_count = group_by_sds.count().reset_index(drop=True)\n        display(' SDS Sum:')\n        display(sds_sum.head())\n        display(' SDS Count: ')\n        display(sds_count.head())\n    return\n\n\ndef split_last_n_by_series_id(df, n):\n    \"\"\"Group df by series identifiers and split on last n rows for each group.\"\"\"\n\n    df_grouped = \\\n        df.sort_values(time_column_name).groupby(time_series_id_column_names,\n            group_keys=False)  # Sort by ascending time\n    df_head = df_grouped.apply(lambda dfg: dfg.iloc[:-n])\n    df_tail = df_grouped.apply(lambda dfg: dfg.iloc[-n:])\n    return (df_head, df_tail)\n\n\ndef APE(actual, pred):\n    \"\"\"\n    Calculate absolute percentage error.\n    Returns a vector of APE values with same length as actual/pred.\n    \"\"\"\n\n    return 100 * np.abs((actual - pred) / actual)\n\n\ndef MAPE(actual, pred):\n    \"\"\"\n    Calculate mean absolute percentage error.\n    Remove NA and values where actual is close to zero\n    \"\"\"\n\n    not_na = ~(np.isnan(actual) | np.isnan(pred))\n    not_zero = ~np.isclose(actual, 0.0)\n    actual_safe = actual[not_na & not_zero]\n    pred_safe = pred[not_na & not_zero]\n    return np.mean(APE(actual_safe, pred_safe))\n  \n\ndef normalize(x, newLowerBound, newUpperBound):\n    # Normalizing from one range to another\n    min = np.min(x)\n    max = np.max(x)\n    range = max - min\n    newRange = newUpperBound - newLowerBound\n\n    return [int(((a - min) / range) * newRange + newLowerBound) for a in x]\n\ndef df_crossjoin(df1, df2, **kwargs):\n    \"\"\"\n    Make a cross join (cartesian product) between two dataframes by using a constant temporary key.\n    Also sets a MultiIndex which is the cartesian product of the indices of the input dataframes.\n    See: https://github.com/pydata/pandas/issues/5401\n    :param df1 dataframe 1\n    :param df1 dataframe 2\n    :param kwargs keyword arguments that will be passed to pd.merge()\n    :return cross join of df1 and df2\n    \"\"\"\n    df1['_tmpkey'] = 1\n    df2['_tmpkey'] = 1\n\n    res = pd.merge(df1, df2, on='_tmpkey', **kwargs).drop('_tmpkey', axis=1)\n    #res.index = pd.MultiIndex.from_product((df1.index, df2.index))\n    res = res.reset_index(drop=True)\n\n    df1.drop('_tmpkey', axis=1, inplace=True)\n    df2.drop('_tmpkey', axis=1, inplace=True)\n\n    return res\n  \ndef movecol(df, cols_to_move=[], ref_col='', place='After'):\n    \n    cols = df.columns.tolist()\n    if place == 'After':\n        seg1 = cols[:list(cols).index(ref_col) + 1]\n        seg2 = cols_to_move\n    if place == 'Before':\n        seg1 = cols[:list(cols).index(ref_col)]\n        seg2 = cols_to_move + [ref_col]\n    \n    seg1 = [i for i in seg1 if i not in seg2]\n    seg3 = [i for i in cols if i not in seg1 + seg2]\n    \n    return(df[seg1 + seg2 + seg3])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e4aa933-d9b5-414c-9266-780bc99ae55b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#!/usr/bin/python\n# -*- coding: utf-8 -*-\nimport pandas as pd\nimport numpy as np\nfrom pandas.tseries.frequencies import to_offset\n\n\ndef align_outputs(\n    y_predicted,\n    X_trans,\n    X_test,\n    y_test,\n    target_column_name,\n    predicted_column_name='predicted',\n    horizon_colname='horizon_origin',\n    ):\n    \"\"\"\n    Demonstrates how to get the output aligned to the inputs\n    using pandas indexes. Helps understand what happened if\n    the output's shape differs from the input shape, or if\n    the data got re-sorted by time and grain during forecasting.\n\n    Typical causes of misalignment are:\n    * we predicted some periods that were missing in actuals -> drop from eval\n    * model was asked to predict past max_horizon -> increase max horizon\n    * data at start of X_test was needed for lags -> provide previous periods\n    \"\"\"\n\n    if horizon_colname in X_trans:\n        df_fcst = pd.DataFrame({predicted_column_name: y_predicted,\n                               horizon_colname: X_trans[horizon_colname]})\n    else:\n        df_fcst = pd.DataFrame({predicted_column_name: y_predicted})\n\n    # y and X outputs are aligned by forecast() function contract\n\n    df_fcst.index = X_trans.index\n\n    # align original X_test to y_test\n\n    X_test_full = X_test.copy()\n    X_test_full[target_column_name] = y_test\n\n    # X_test_full's index does not include origin, so reset for merge\n\n    df_fcst.reset_index(inplace=True)\n    X_test_full = X_test_full.reset_index().drop(columns='index')\n    together = df_fcst.merge(X_test_full, how='right')\n\n    # drop rows where prediction or actuals are nan\n    # happens because of missing actuals\n    # or at edges of time due to lags/rolling windows\n\n    clean = together[together[[target_column_name,\n                     predicted_column_name]].notnull().all(axis=1)]\n    return clean\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2a0ee83-41e3-4aff-8e7a-4d40534fcbe5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c2f3f09-10d3-41b8-b21c-17ddad269261"}},"outputs":[],"execution_count":0}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.9","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3.6 - AzureML","language":"python","name":"python3-azureml"},"kernel_info":{"name":"python3-azureml"},"application/vnd.databricks.v1+notebook":{"notebookName":"automl_helper","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4080261228198117},"nteract":{"version":"nteract-front-end@1.0.0"}},"nbformat":4,"nbformat_minor":0}
