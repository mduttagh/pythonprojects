{"cells":[{"cell_type":"code","source":["#define widgets - NEED TO DEFINE IT ONCE\n# dbutils.widgets.text(\"environment\", \"\",\"\")\n# dbutils.widgets.text(\"system_name\", \"\",\"\")\n# dbutils.widgets.text(\"data_load_type\", \"\",\"\")\n\n# To remove unnecessary dbutils\n# dbutils.widgets.removeAll()\n\n#dynamic variables (pass it from ADF)\nenvironment = dbutils.widgets.get(\"environment\")\nsystem_name = dbutils.widgets.get(\"system_name\")\ndata_load_type = dbutils.widgets.get(\"data_load_type\") # Full/Incremental\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ffdb3205-866c-4b97-954a-cd7ddb298411"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{"data_load_type":"Full","system_name":"bimodelapi","environment":"prod"}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%run ../../bi_config/pbi_common"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Common library","showTitle":true,"inputWidgets":{},"nuid":"7c47aa67-c905-49e9-9cde-984d5e82e52f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{"environment":"prod"}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{"environment":"prod"}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{"environment":"prod"}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# ============================ get raw data for opportunity entity======================= #\nimport os\nfrom datetime import date\nfrom datetime import date, timedelta\nfrom dateutil.relativedelta import relativedelta\n\nprint(\"Downloading raw data\")\n\n#static variables\ng_bi_config_parameters_path = \"/mnt/\"+ environment + \"/gold/g_bi_config_parameters\"\nfull_load_date = datetime(1970, 3, 1)\nokta_end_poiont = \"auth/getapitoken\"\nend_point_tag = \"summary\"\n#reading config table\ndf_bi_configuration  = spark.read.format(\"delta\").load(g_bi_config_parameters_path)\ndf_d365_bi_configuration = df_bi_configuration.filter(df_bi_configuration.SystemName == \"bimodelapi\")\ndf_bi_configuration  = df_bi_configuration.filter(df_bi_configuration.SystemName == \"bimodelapi\")  \n\n#initializing config parameter values\nservice_account_name =  df_bi_configuration.filter(df_bi_configuration.ParameterName == \"service_account_name\")\\\n                                               .select(\"ParameterValue\")\\\n                                               .collect()[0][0]\n\nbronze_folder_path =  df_bi_configuration.filter(df_bi_configuration.ParameterName == \"bronze_folder_path\")\\\n                                               .select(\"ParameterValue\")\\\n                                               .collect()[0][0]\n\nsilver_folder_path =  df_bi_configuration.filter(df_bi_configuration.ParameterName == \"silver_folder_path\")\\\n                                               .select(\"ParameterValue\")\\\n                                               .collect()[0][0]\n\ngold_folder_path =  df_bi_configuration.filter(df_bi_configuration.ParameterName == \"gold_folder_path\")\\\n                                               .select(\"ParameterValue\")\\\n                                               .collect()[0][0]\n\nsource_path = df_bi_configuration.filter(df_bi_configuration.ParameterName == \"source_path\")\\\n                                               .select(\"ParameterValue\")\\\n                                               .collect()[0][0]\n\nconf_threshold_value =  df_bi_configuration.filter(df_bi_configuration.ParameterName == \"threshold\")\\\n                                               .select(\"ParameterValue\")\\\n                                               .collect()[0][0]\n\npowerbi_username =  df_bi_configuration.filter(df_bi_configuration.ParameterName == \"powerbi_username\")\\\n                                               .select(\"ParameterValue\")\\\n                                               .collect()[0][0]\n\nocp_apim_subscription_key =  df_bi_configuration.filter(df_bi_configuration.ParameterName == \"ocp_apim_subscription_key\")\\\n                                               .select(\"ParameterValue\")\\\n                                               .collect()[0][0]\n\ncurated_folder_path   =  df_bi_configuration.filter(df_bi_configuration.ParameterName == \"curated_folder_path\")\\\n                                                     .select(\"ParameterValue\")\\\n                                                     .collect()[0][0]\n#initialize storage account \nstorage_name = df_bi_configuration.filter(df_bi_configuration.ParameterName == \"storage_name\")\\\n                                               .select(\"ParameterValue\")\\\n                                               .collect()[0][0]\nif environment == \"prod\":\n  pbi_api_password = dbutils.secrets.get(scope = \"kv-bi-prod-01-secrets\", key = \"pbi-biuser-ideocom-key\")\n  storage_key  = dbutils.secrets.get(scope=\"kv-bi-prod-01-secrets\", key=\"databricks-{}-storage-key\".format(environment))\nelse:\n  pbi_api_password = dbutils.secrets.get(scope = \"kv-bi-devqa-01-secrets\", key = \"pbi-biuser-ideocom-key\")\n  storage_key  = dbutils.secrets.get(scope=\"kv-bi-devqa-01-secrets\", key=\"databricks-{}-storage-key\".format(environment))\n  \nspark.conf.set(\"fs.azure.account.key.{}.blob.core.windows.net\".format(storage_name),storage_key)\n\nsource_system_id = system_name + \"/\" + end_point_tag\n\n#setting delta tables path\ng_automljobruninfo_path = gold_folder_path + \"/automljobruninfo\"  # Story # 3181\n  \n#reading job run info\ndf_job_info = spark.read.format(\"delta\").load(g_automljobruninfo_path) # Story # 3181\n\n#initialize batch id and batch start date time variables\nbatch_id = df_job_info.agg({\"batch_id\" : \"max\"}).collect()[0][0]\nbatch_start_datetime = df_job_info.agg({\"batch_start_datetime\" : \"max\"}).collect()[0][0]\nlast_updated_datetime = df_job_info.agg({\"last_updated_datetime\" : \"max\"}).collect()[0][0]\n\nnotebook = os.path.basename(getNotebookPath())\n\ntry:   \n  #api call for okta authorization\n  subscription_key = \"\"\n  query_params = {\"username\": powerbi_username ,\"password\": pbi_api_password}\n  subscription_key = pbi_post('{}'.format(okta_end_poiont),ocp_apim_subscription_key,query_params)\n  current_date = date.today().strftime(\"(%Y,%m,%d)\")\n  from_date = \"(2017,01,01)\"\n  #to_date = \"(9999,12,31)\"\n  opp_from_date = \"(2018,07,01)\"\n  #cur_month_eod = date.today()\n  #cur_month_eod = date(cur_month_eod.year + (cur_month_eod.month == 12),(cur_month_eod.month + 1 if cur_month_eod.month < 12 else 1), 1) - timedelta(1)\n  #cur_month_eod = cur_month_eod.strftime(\"(%Y,%m,%d)\")\n  eom_date = date.today() + relativedelta(months=+11)\n  eom_date = date(eom_date.year + (eom_date.month == 12),(eom_date.month + 1 if eom_date.month < 12 else 1), 1) - timedelta(1)\n  eom_date = eom_date.strftime(\"(%Y,%m,%d)\")\n  #Call summary/gl\n  entity_name =  \"gl\"\n  entity_responses = []\n  # Strory # 316 IncludeManagingStudioDept or IncludeContributingStudioDept parameter. Automl - add exclude Studio-Department = Venture Fund from apisummary\n  if data_load_type == 'Full':\n    # ts 2021-06-02 : changed todate filter from \"(9999,12,31)\" to current date + 11 months \n    query_params = {\"DrillDownOptions\" : \"Managing Sub Region\",\"PostingLayerNames\" : \"Current, Custom layer 1\", \"ExcludeSubTypeCodes\" : \"2020, 2050, 2055, 2060\",\\\n                    \"FromDate\": from_date, \"ToDate\" : eom_date, \"ReportCurrencyCode\" : \"USD\", \"IncludeManagingStudioDept\":\"true\"}\n  else:\n    query_params = {\"DrillDownOptions\" : \"Managing Sub Region\",\"PostingLayerNames\" : \"Current, Custom layer 1\", \"ExcludeSubTypeCodes\" : \"2020, 2050, 2055, 2060\", \\\n                    \"FromDate\": from_date, \"ToDate\" : eom_date,  \"ReportCurrencyCode\" : \"USD\", \"IncludeManagingStudioDept\":\"true\"}\n   \n  responses = pbi_post('{}/{}'.format(end_point_tag,entity_name), ocp_apim_subscription_key,query_params,subscription_key)\n  for res in responses:\n    if res.status_code not in [200, 201, 204]:\n      raise Exception(res.json().get('error'))\n    \n    entity_responses.append(res.json())  \n  \n  #storing data into json file\n  gl_filepath = write_to_json(get_filepath(source_path, source_system_id, entity_name, batch_id), entity_responses)\n  \n  # Call summary/pipeline\n  entity_name = \"pipeline\"\n  entity_responses = []\n  # Strory # 316 IncludeManagingStudioDept or IncludeContributingStudioDept parameter. Automl - add exclude Studio-Department = Venture Fund from apisummary\n  if data_load_type == 'Full':\n    query_params = {\"DrillDownOptions\" : \"Contrib Sub Region, Relative Snapshot Month Offset, Relative Month Offset, Pipeline Type\",\\\n                    \"FromDate\": from_date, \"ToDate\" : eom_date, \"FromSnapshotDate\":from_date,  \"ToSnapshotDate\" : current_date, \"IncludeContributingStudioDept\":\"true\"}\n  else:\n    query_params = {\"DrillDownOptions\" : \"Contrib Sub Region, Relative Snapshot Month Offset, Relative Month Offset, Pipeline Type\", \\\n                    \"FromDate\": from_date, \"ToDate\" : eom_date, \"FromSnapshotDate\":from_date,  \"ToSnapshotDate\" : current_date,\"IncludeContributingStudioDept\":\"true\"}    \n\n  responses = pbi_post('{}/{}'.format(end_point_tag,entity_name), ocp_apim_subscription_key, query_params,subscription_key)\n  for res in responses:\n    if res.status_code not in [200, 201, 204]:\n      raise Exception(res.json().get('error'))\n    \n    entity_responses.append(res.json())\n      \n  #storing data into json file\n  pipeline_filepath = write_to_json(get_filepath(source_path, source_system_id, entity_name, batch_id), entity_responses)\n\n  #Call summary/opportunities 1\n  entity_name = \"opportunities\"\n  entity_responses = []\n  # Strory # 316 IncludeManagingStudioDept or IncludeContributingStudioDept parameter. Automl - add exclude Studio-Department = Venture Fund from apisummary\n  if data_load_type == 'Full':\n    # ts 2021-06-02 : changed todate filter from \"(9999,12,31)\" to current date + 11 months \n    query_params = {\"DrillDownOptions\" : \"Managing Sub Region\", \"OpportunityStatus\" : \"Open\",\"FromDate\": opp_from_date, \"ToDate\" : eom_date, \"IncludeManagingStudioDept\":\"true\"}\n  else:\n    # ts 2021-06-02 : changed todate filter from \"(9999,12,31)\" to current date + 11 months \n    query_params = {\"DrillDownOptions\" : \"Managing Sub Region\", \"OpportunityStatus\" : \"Open\",\"FromDate\": opp_from_date, \"ToDate\" : eom_date, \"IncludeManagingStudioDept\":\"true\"}\n\n  responses = pbi_post('{}/{}'.format(end_point_tag,entity_name),ocp_apim_subscription_key, query_params,subscription_key)\n  for res in responses:\n    if res.status_code not in [200, 201, 204]:\n      raise Exception(res.json().get('error'))\n    \n    entity_responses.append(res.json())\n\n  \n  #storing data into json file\n  opportunities_filepath = write_to_json(get_filepath(source_path, source_system_id, entity_name, batch_id), entity_responses)\n  print(opportunities_filepath)\n  \n  # Call summary/projects\n  entity_name = \"projects\"\n  entity_responses = []\n   # Strory # 3162 IncludeManagingStudioDept or IncludeContributingStudioDept parameter. Automl - add exclude Studio-Department = Venture Fund from apisummary\n  if data_load_type == 'Full':\n    # ts 2021-06-02 : changed todate filter from \"(9999,12,31)\" to current date + 11 months \n    query_params = {\"DrillDownOptions\" : \"Managing Sub Region\", \"ProjectStatus\" : \"In Planning, Pending, Okay to Start, Active, Delivered\",\"FromDate\": from_date, \"ToDate\" : eom_date,\"IncludeManagingStudioDept\":\"true\"}\n  else:\n    # ts 2021-06-02 : changed todate filter from \"(9999,12,31)\" to current date + 11 months\n    query_params = {\"DrillDownOptions\" : \"Managing Sub Region\", \"ProjectStatus\" : \"In Planning, Pending, Okay to Start, Active, Delivered\",\"FromDate\": from_date, \"ToDate\" : eom_date,\"IncludeManagingStudioDept\":\"true\"} \n  \n  responses = pbi_post('{}/{}'.format(end_point_tag,entity_name), ocp_apim_subscription_key, query_params,subscription_key)\n  for res in responses:\n    if res.status_code not in [200, 201, 204]:\n      raise Exception(res.json().get('error'))\n    \n    entity_responses.append(res.json())\n    \n  #storing data into json file\n  projects_filepath = write_to_json(get_filepath(source_path, source_system_id, entity_name, batch_id), entity_responses)\n  \n  #Call summary/headcount\n  entity_name = \"headcount\"\n  entity_responses = []\n  # Strory # 3162 IncludeManagingStudioDept or IncludeContributingStudioDept parameter. Automl - add exclude Studio-Department = Venture Fund from apisummary\n  if data_load_type == 'Full':\n    # ts 2021-06-02 : changed todate filter from \"(9999,12,31)\" to current date + 11 months\n    \n    query_params = {\"DrillDownOptions\" : \"Contrib Sub Region, Journey Level, Billable\", \"FromDate\": from_date, \"ToDate\" : eom_date,\"IncludeContributingStudioDept\":\"true\"}\n  else:\n    # ts 2021-06-02 : changed todate filter from \"(9999,12,31)\" to current date + 11 months\n    query_params = {\"DrillDownOptions\" : \"Contrib Sub Region, Journey Level, Billable\", \"FromDate\": from_date, \"ToDate\" : eom_date,\"IncludeContributingStudioDept\":\"true\"}  \n\n  responses = pbi_post('{}/{}'.format(end_point_tag,entity_name), ocp_apim_subscription_key, query_params,subscription_key)\n  for res in responses:\n    if res.status_code not in [200, 201, 204]:\n      raise Exception(res.json().get('error'))\n    \n    entity_responses.append(res.json())\n    \n  #storing data into json file\n  headcount_filepath = write_to_json(get_filepath(source_path, source_system_id, entity_name, batch_id), entity_responses)\n  \n  # Call summary/project allocations\n  entity_name = \"project allocations\"\n  entity_responses = []\n  #Strory # 3162 IncludeManagingStudioDept or IncludeContributingStudioDept parameter. Automl - add exclude Studio-Department = Venture Fund from apisummary\n  if data_load_type == 'Full':\n    # ts 2021-06-02 : changed todate filter from current month eod to current date + 11 months\n    query_params = {\"DrillDownOptions\" : \"Contrib Sub Region, Journey Level\", \"FromDate\": from_date, \"ToDate\" : eom_date,\"IncludeContributingStudioDept\":\"true\"}\n  else:\n    # ts 2021-06-02 : changed todate filter from current month eod to current date + 11 months\n    query_params = {\"DrillDownOptions\" : \"Contrib Sub Region, Journey Level\", \"FromDate\": from_date, \"ToDate\" : eom_date,\"IncludeContributingStudioDept\":\"true\"}\n\n  responses = pbi_post('{}/{}'.format(end_point_tag,entity_name), ocp_apim_subscription_key, query_params,subscription_key)\n  for res in responses:\n    if res.status_code not in [200, 201, 204]:\n      raise Exception(res.json().get('error'))\n    \n    entity_responses.append(res.json())\n    \n  #storing data into json file\n  project_allocations_filepath = write_to_json(get_filepath(source_path, source_system_id, entity_name, batch_id), entity_responses)\n  \n  #Call summary/pipeline trend\n  entity_name = \"pipeline trend\"\n  entity_responses = []\n  # Strory # 3162 IncludeManagingStudioDept or IncludeContributingStudioDept parameter. Automl - add exclude Studio-Department = Venture Fund from apisummary\n  if data_load_type == 'Full':\n    query_params = {\"DrillDownOptions\" : \"Contrib Sub Region\", \"FromSnapshotDate\":from_date, \"ToSnapshotDate\" : current_date,\"IncludeContributingStudioDept\":\"true\"}\n  else:\n    query_params = {\"DrillDownOptions\" : \"Contrib Sub Region\", \"FromSnapshotDate\":from_date, \"ToSnapshotDate\" : current_date,\"IncludeContributingStudioDept\":\"true\"}\n\n  responses = pbi_post('{}/{}'.format(end_point_tag,entity_name),ocp_apim_subscription_key,query_params,subscription_key)\n  for res in responses:\n    if res.status_code not in [200, 201, 204]:\n      raise Exception(res.json().get('error'))\n      \n    entity_responses.append(res.json())\n    \n  #storing data into json file\n  pipeline_trend_filepath = write_to_json(get_filepath(source_path, source_system_id, entity_name, batch_id), entity_responses)\n\nexcept Exception as error:\n  print(error)\n  log_error(\"{} {}\".format(notebook, error)) #log error in sentry\n  #raise dbutils.notebook.exit(error) #raise the exception\n  raise error #raise the exception"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Download raw data","showTitle":true,"inputWidgets":{},"nuid":"d556dd08-ba51-414f-be20-41ab2fc35277"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df_d365_bi_configuration","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"ID","nullable":true,"type":"integer"},{"metadata":{},"name":"EnvironmentName","nullable":true,"type":"string"},{"metadata":{},"name":"SystemName","nullable":true,"type":"string"},{"metadata":{},"name":"ParameterName","nullable":true,"type":"string"},{"metadata":{},"name":"ParameterValue","nullable":true,"type":"string"},{"metadata":{},"name":"IBICreatedBy","nullable":true,"type":"string"},{"metadata":{},"name":"IBIUpdatedBy","nullable":true,"type":"string"},{"metadata":{},"name":"IBICreatedDate","nullable":true,"type":"timestamp"},{"metadata":{},"name":"IBIUpdatedDate","nullable":true,"type":"timestamp"}],"type":"struct"},"tableIdentifier":null},{"name":"df_bi_configuration","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"ID","nullable":true,"type":"integer"},{"metadata":{},"name":"EnvironmentName","nullable":true,"type":"string"},{"metadata":{},"name":"SystemName","nullable":true,"type":"string"},{"metadata":{},"name":"ParameterName","nullable":true,"type":"string"},{"metadata":{},"name":"ParameterValue","nullable":true,"type":"string"},{"metadata":{},"name":"IBICreatedBy","nullable":true,"type":"string"},{"metadata":{},"name":"IBIUpdatedBy","nullable":true,"type":"string"},{"metadata":{},"name":"IBICreatedDate","nullable":true,"type":"timestamp"},{"metadata":{},"name":"IBIUpdatedDate","nullable":true,"type":"timestamp"}],"type":"struct"},"tableIdentifier":null},{"name":"df_job_info","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"job_name","nullable":true,"type":"string"},{"metadata":{},"name":"batch_id","nullable":true,"type":"long"},{"metadata":{},"name":"batch_start_datetime","nullable":true,"type":"timestamp"},{"metadata":{},"name":"start_datetime","nullable":true,"type":"timestamp"},{"metadata":{},"name":"end_datetime","nullable":true,"type":"timestamp"},{"metadata":{},"name":"last_updated_datetime","nullable":true,"type":"timestamp"},{"metadata":{},"name":"status","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":"dbfs:/mnt/prod/gold/automljobruninfo"}],"data":"<div class=\"ansiout\">Downloading raw data\nTotal Row:summary/gl....637\nWrote 388850 bytes.\nTotal Row:summary/pipeline....118669\nretry page no:  9\npbi_post_retry called\nstatus: False\nretry page no:  58\npbi_post_retry called\nstatus: False\nretry page no:  61\npbi_post_retry called\nstatus: False\nWrote 100715663 bytes.\nTotal Row:summary/opportunities....491\nWrote 341566 bytes.\n/mnt/prod/raw/enterprise/bimodelapi/summary/opportunities/2022/02/25/opportunities_20220225151400.json\nTotal Row:summary/projects....612\nWrote 374200 bytes.\nTotal Row:summary/headcount....5859\nWrote 3823799 bytes.\nTotal Row:summary/project allocations....3104\nWrote 1779070 bytes.\nTotal Row:summary/pipeline trend....1635\nretry page no:  1\npbi_post_retry called\nstatus: False\nWrote 878632 bytes.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Downloading raw data\nTotal Row:summary/gl....637\nWrote 388850 bytes.\nTotal Row:summary/pipeline....118669\nretry page no:  9\npbi_post_retry called\nstatus: False\nretry page no:  58\npbi_post_retry called\nstatus: False\nretry page no:  61\npbi_post_retry called\nstatus: False\nWrote 100715663 bytes.\nTotal Row:summary/opportunities....491\nWrote 341566 bytes.\n/mnt/prod/raw/enterprise/bimodelapi/summary/opportunities/2022/02/25/opportunities_20220225151400.json\nTotal Row:summary/projects....612\nWrote 374200 bytes.\nTotal Row:summary/headcount....5859\nWrote 3823799 bytes.\nTotal Row:summary/project allocations....3104\nWrote 1779070 bytes.\nTotal Row:summary/pipeline trend....1635\nretry page no:  1\npbi_post_retry called\nstatus: False\nWrote 878632 bytes.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import explode, col,when,udf,split,regexp_replace,lit\nfrom pyspark.sql.types import ArrayType, IntegerType,StringType,TimestampType,DecimalType,DateType\nimport pandas as pd\n#initializing dataframe from API data \nprint(\"Converting to pandas...\")\n\ntry:\n  \n  #Revenue\n  #gl_filepath = \"/mnt/dev/raw/enterprise/bimodelapi/summary/gl/2021/05/05/gl_20210505131204.json\"\n  df_gl = spark.read.option(\"multiLine\",\"true\").json(gl_filepath)\n\n  #set varivable for Audit Control \n  gl_min_records = df_gl.agg({\"TotalRows\" : \"min\"}).collect()[0][0]\n  gl_max_records = df_gl.agg({\"TotalRows\" : \"max\"}).collect()[0][0]\n\n  df_gl = df_gl.withColumn(\"data\",explode(col('data'))).select(\"data.*\")\n  df_gl = df_gl.select(\"Date[End of Month]\", \"Managing Studio Location[Managing Sub Region Code]\", \"[Revenue]\")\n\n  df_gl = df_gl.withColumnRenamed(\"Date[End of Month]\",\"End_of_Month\")\\\n               .withColumnRenamed(\"Managing Studio Location[Managing Sub Region Code]\",\"Managing Sub Region Code\")\\\n               .withColumnRenamed(\"[Revenue]\",\"Revenue\")\n\n  df_gl = df_gl.withColumn(\"End_of_Month\",  df_gl[\"End_of_Month\"].cast(DateType()))\n  df_gl = df_gl.withColumn(\"Managing Sub Region Code\",  df_gl[\"Managing Sub Region Code\"].cast(StringType()))\n  df_gl = df_gl.withColumn(\"Revenue\",regexp_replace(\"Revenue\", \",\", ''))\n  df_gl = df_gl.withColumn(\"Revenue\",  df_gl[\"Revenue\"].cast(DecimalType(32,6)))\n\n  #set varivable for Audit Control \n  gl_record_count = df_gl.agg({\"*\" : \"count\"}).collect()[0][0]\n  #Audit Control \n  check_audit_contorls(\"gl\",gl_record_count,gl_min_records,gl_max_records)\n\n  #Opp\n  #opportunities_filepath = \"/mnt/dev/raw/enterprise/bimodelapi/summary/opportunities/2021/05/05/opportunities_20210505131204.json\"\n  df_opportunities = spark.read.option(\"multiLine\",\"true\").json(opportunities_filepath)\n  #set varivable for Audit Control \n  opp_min_records = df_opportunities.agg({\"TotalRows\" : \"min\"}).collect()[0][0]\n  opp_max_records = df_opportunities.agg({\"TotalRows\" : \"max\"}).collect()[0][0]\n\n  df_opportunities = df_opportunities.withColumn(\"data\",explode(col('data'))).select(\"data.*\")\n  df_opportunities = df_opportunities.select(\"Date[End of Month]\", \"Managing Studio Location[Managing Sub Region Code]\", \\\n                                             \"[Opportunity_Period_Count]\",\"[Opportunity_Count]\",\"[Current_Opp_Period_Value]\", \\\n                                             \"[Opportunity_Value]\", \"[Win_Rate]\",\"[Current_Opp_Period_Count]\",\"[Opportunity_Period_Value]\")\n\n  df_opportunities = df_opportunities.withColumnRenamed(\"Date[End of Month]\",\"End_of_Month\")\\\n                                     .withColumnRenamed(\"Managing Studio Location[Managing Sub Region Code]\",\"Managing Sub Region Code\")\\\n                                     .withColumnRenamed(\"[Opportunity_Period_Count]\",\"Opportunity Period Count\")\\\n                                     .withColumnRenamed(\"[Opportunity_Count]\",\"Opportunity Count\")\\\n                                     .withColumnRenamed(\"[Current_Opp_Period_Value]\",\"Current Opp Period Value\")\\\n                                     .withColumnRenamed(\"[Opportunity_Value]\",\"Opportunity Value\")\\\n                                     .withColumnRenamed(\"[Win_Rate]\",\"Win Rate\")\\\n                                     .withColumnRenamed(\"[Current_Opp_Period_Count]\",\"Current Opp Period Count\")\\\n                                     .withColumnRenamed(\"[Opportunity_Period_Value]\",\"Opportunity Period Value\")\n  \n  df_opportunities = df_opportunities.withColumn(\"End_of_Month\",  df_opportunities[\"End_of_Month\"].cast(DateType()))\n  df_opportunities = df_opportunities.withColumn(\"Managing Sub Region Code\",  df_opportunities[\"Managing Sub Region Code\"].cast(StringType()))\n  df_opportunities = df_opportunities.withColumn(\"Opportunity Period Count\",regexp_replace(\"Opportunity Period Count\", \",\", ''))\n  df_opportunities = df_opportunities.withColumn(\"Opportunity Period Count\",  df_opportunities[\"Opportunity Period Count\"].cast(IntegerType()))\n  df_opportunities = df_opportunities.withColumn(\"Opportunity Count\",regexp_replace(\"Opportunity Count\", \",\", ''))\n  df_opportunities = df_opportunities.withColumn(\"Opportunity Count\",  df_opportunities[\"Opportunity Count\"].cast(IntegerType()))\n  df_opportunities = df_opportunities.withColumn(\"Current Opp Period Value\",regexp_replace(\"Current Opp Period Value\", \",\", ''))\n  df_opportunities = df_opportunities.withColumn(\"Current Opp Period Value\",  df_opportunities[\"Current Opp Period Value\"].cast(DecimalType(32,6)))\n  df_opportunities = df_opportunities.withColumn(\"Opportunity Value\",regexp_replace(\"Opportunity Value\", \",\", ''))\n  df_opportunities = df_opportunities.withColumn(\"Opportunity Value\",  df_opportunities[\"Opportunity Value\"].cast(DecimalType(32,6)))\n  df_opportunities = df_opportunities.withColumn(\"Current Opp Period Count\",regexp_replace(\"Current Opp Period Count\", \",\", ''))\n  df_opportunities = df_opportunities.withColumn(\"Current Opp Period Count\",  df_opportunities[\"Current Opp Period Count\"].cast(IntegerType()))\n  df_opportunities = df_opportunities.withColumn(\"Opportunity Period Value\",regexp_replace(\"Opportunity Period Value\", \",\", ''))\n  df_opportunities = df_opportunities.withColumn(\"Opportunity Period Value\",  df_opportunities[\"Opportunity Period Value\"].cast(DecimalType(32,6)))\n  df_opportunities = df_opportunities.withColumn(\"Win Rate\",regexp_replace(\"Win Rate\", \"%\", ''))\n  df_opportunities = df_opportunities.withColumn(\"Win Rate\",  df_opportunities[\"Win Rate\"].cast(DecimalType(8,4)) * lit(0.01))\n\n  #set varivable for Audit Control \n  opp_record_count = df_opportunities.agg({\"*\" : \"count\"}).collect()[0][0]\n  #Audit Control \n  check_audit_contorls(\"opportunities\",opp_record_count,opp_min_records,opp_max_records)\n\n\n  #project\n  #projects_filepath = \"/mnt/dev/raw/enterprise/bimodelapi/summary/projects/2021/05/05/projects_20210505131204.json\"\n  df_projects = spark.read.option(\"multiLine\",\"true\").json(projects_filepath)\n  #set varivable for Audit Control \n  projects_min_records = df_projects.agg({\"TotalRows\" : \"min\"}).collect()[0][0]\n  projects_max_records = df_projects.agg({\"TotalRows\" : \"max\"}).collect()[0][0]\n  \n  df_projects = df_projects.withColumn(\"data\",explode(col('data'))).select(\"data.*\")\n  df_projects = df_projects.select(\"Date[End of Month]\", \"Managing Studio Location[Managing Sub Region Code]\", \\\n                                             \"[Project Period Count]\",\"[Project Count]\",\"[Project Period Price]\", \\\n                                             \"[Project Price]\", \"[Conversions]\")\n\n  df_projects = df_projects.withColumnRenamed(\"Date[End of Month]\",\"End_of_Month\")\\\n                           .withColumnRenamed(\"Managing Studio Location[Managing Sub Region Code]\",\"Managing Sub Region Code\")\\\n                           .withColumnRenamed(\"[Project Period Count]\",\"Project Period Count\")\\\n                           .withColumnRenamed(\"[Project Count]\",\"Project Count\")\\\n                           .withColumnRenamed(\"[Project Period Price]\",\"Project Period Price\")\\\n                           .withColumnRenamed(\"[Project Price]\",\"Project Price\")\\\n                           .withColumnRenamed(\"[Conversions]\",\"Conversions\")\n\n  df_projects = df_projects.withColumn(\"End_of_Month\",  df_projects[\"End_of_Month\"].cast(DateType()))\n  df_projects = df_projects.withColumn(\"Managing Sub Region Code\",  df_projects[\"Managing Sub Region Code\"].cast(StringType()))\n  df_projects = df_projects.withColumn(\"Project Period Count\",regexp_replace(\"Project Period Count\", \",\", ''))\n  df_projects = df_projects.withColumn(\"Project Period Count\",  df_projects[\"Project Period Count\"].cast(IntegerType()))\n  df_projects = df_projects.withColumn(\"Project Count\",regexp_replace(\"Project Count\", \",\", ''))\n  df_projects = df_projects.withColumn(\"Project Count\",  df_projects[\"Project Count\"].cast(IntegerType()))\n  df_projects = df_projects.withColumn(\"Project Period Price\",regexp_replace(\"Project Period Price\", \",\", ''))\n  df_projects = df_projects.withColumn(\"Project Period Price\",  df_projects[\"Project Period Price\"].cast(DecimalType(32,6)))\n  df_projects = df_projects.withColumn(\"Project Price\",regexp_replace(\"Project Price\", \",\", ''))\n  df_projects = df_projects.withColumn(\"Project Price\",  df_projects[\"Project Price\"].cast(DecimalType(32,6)))\n  df_projects = df_projects.withColumn(\"Conversions\",regexp_replace(\"Conversions\", \",\", ''))\n  df_projects = df_projects.withColumn(\"Conversions\",  df_projects[\"Conversions\"].cast(DecimalType(32,6)))\n  \n  #set varivable for Audit Control \n  projects_record_count = df_projects.agg({\"*\" : \"count\"}).collect()[0][0]\n  #Audit Control \n  check_audit_contorls(\"projects\",projects_record_count,projects_min_records,projects_max_records)\n\n  #Talent\n  #headcount_filepath = \"/mnt/dev/raw/enterprise/bimodelapi/summary/headcount/2021/05/14/headcount_20210514193743.json\"\n  df_headcount = spark.read.option(\"multiLine\",\"true\").json(headcount_filepath)\n  \n  #set varivable for Audit Control \n  headcount_min_records = df_headcount.agg({\"TotalRows\" : \"min\"}).collect()[0][0]\n  headcount_max_records = df_headcount.agg({\"TotalRows\" : \"max\"}).collect()[0][0]\n  \n  df_headcount = df_headcount.withColumn(\"data\",explode(col('data'))).select(\"data.*\")\n  df_headcount = df_headcount.select(\"Date[End of Month]\", \"Contributing Studio Location[Contrib Sub Region Code]\", \\\n                                     \"Journey Level[Journey Level]\",\"Billable[Billable]\",\"[Headcount Value]\",\"[Headcount Contingent]\")\n\n  df_headcount = df_headcount.withColumnRenamed(\"Date[End of Month]\",\"End_of_Month\")\\\n                           .withColumnRenamed(\"Contributing Studio Location[Contrib Sub Region Code]\",\"Contrib Sub Region Code\")\\\n                           .withColumnRenamed(\"Journey Level[Journey Level]\",\"Journey Level\")\\\n                           .withColumnRenamed(\"Billable[Billable]\",\"Billable\")\\\n                           .withColumnRenamed(\"[Headcount Value]\",\"Headcount\")\\\n                           .withColumnRenamed(\"[Headcount Contingent]\",\"Headcount Contingent\")\n\n  df_headcount = df_headcount.withColumn(\"End_of_Month\",  df_headcount[\"End_of_Month\"].cast(DateType()))\n  df_headcount = df_headcount.withColumn(\"Contrib Sub Region Code\",  df_headcount[\"Contrib Sub Region Code\"].cast(StringType()))\n  df_headcount = df_headcount.withColumn(\"Journey Level\",  df_headcount[\"Journey Level\"].cast(StringType()))\n  df_headcount = df_headcount.withColumn(\"Billable\",  df_headcount[\"Billable\"].cast(StringType()))\n  df_headcount = df_headcount.withColumn(\"Headcount\",regexp_replace(\"Headcount\", \",\", ''))\n  df_headcount = df_headcount.withColumn(\"Headcount\",  df_headcount[\"Headcount\"].cast(DecimalType(32,6)))\n  df_headcount = df_headcount.withColumn(\"Headcount Contingent\",regexp_replace(\"Headcount Contingent\", \",\", ''))\n  df_headcount = df_headcount.withColumn(\"Headcount Contingent\",  df_headcount[\"Headcount Contingent\"].cast(DecimalType(32,6)))\n  \n  #set varivable for Audit Control \n  headcount_record_count = df_headcount.agg({\"*\" : \"count\"}).collect()[0][0]\n  #Audit Control \n  check_audit_contorls(\"headcount\",headcount_record_count,headcount_min_records,headcount_max_records)\n  \n  #project actual\n  #project_allocations_filepath = \"/mnt/dev/raw/enterprise/bimodelapi/summary/project allocations/2021/05/05/project allocations_20210505131204.json\"\n  df_project_allocations = spark.read.option(\"multiLine\",\"true\").json(project_allocations_filepath)\n  #set varivable for Audit Control \n  project_allocations_min_records = df_project_allocations.agg({\"TotalRows\" : \"min\"}).collect()[0][0]\n  project_allocations_max_records = df_project_allocations.agg({\"TotalRows\" : \"max\"}).collect()[0][0]\n\n  df_project_allocations = df_project_allocations.withColumn(\"data\",explode(col('data'))).select(\"data.*\")\n  df_project_allocations= df_project_allocations.select(\"Date[End of Month]\", \"Contributing Studio Location[Contrib Sub Region Code]\",\\\n                                     \"Journey Level[Journey Level]\",\"[Utilization Billable]\",\"[Nominal Hours]\")\n\n  df_project_allocations= df_project_allocations.withColumnRenamed(\"Date[End of Month]\",\"End_of_Month\")\\\n                           .withColumnRenamed(\"Contributing Studio Location[Contrib Sub Region Code]\",\"Contrib Sub Region Code\")\\\n                           .withColumnRenamed(\"Journey Level[Journey Level]\",\"Journey Level\")\\\n                           .withColumnRenamed(\"[Utilization Billable]\",\"Utilization Billable\")\\\n                           .withColumnRenamed(\"[Nominal Hours]\",\"Nominal_Hours\")\n\n  df_project_allocations= df_project_allocations.withColumn(\"End_of_Month\",  df_project_allocations[\"End_of_Month\"].cast(DateType()))\n  df_project_allocations= df_project_allocations.withColumn(\"Contrib Sub Region Code\",  df_project_allocations[\"Contrib Sub Region Code\"].cast(StringType()))\n  df_project_allocations= df_project_allocations.withColumn(\"Journey Level\",  df_project_allocations[\"Journey Level\"].cast(StringType()))\n  df_project_allocations = df_project_allocations.withColumn(\"Utilization Billable\",regexp_replace(\"Utilization Billable\", \"%\", ''))\n  df_project_allocations = df_project_allocations.withColumn(\"Utilization Billable\",  df_project_allocations[\"Utilization Billable\"].cast(DecimalType(8,4)) * lit(0.01))\n  df_project_allocations = df_project_allocations.withColumn(\"Nominal_Hours\",regexp_replace(\"Nominal_Hours\", \",\", ''))\n  df_project_allocations = df_project_allocations.withColumn(\"Nominal_Hours\", df_project_allocations[\"Nominal_Hours\"].cast(DecimalType(32,6)))\n\n\n  #set varivable for Audit Control \n  project_allocations_record_count = df_project_allocations.agg({\"*\" : \"count\"}).collect()[0][0]\n  #Audit Control \n  check_audit_contorls(\"project_allocations\",project_allocations_record_count,project_allocations_min_records,project_allocations_max_records)\n  \n  #pipeine\n  #pipeline_filepath = \"/mnt/dev/raw/enterprise/bimodelapi/summary/pipeline/2021/05/05/pipeline_20210505131204.json\"\n  #pipeline_filepath =  \"/mnt/dev/raw/enterprise/bimodelapi/summary/pipeline/2021/04/23/pipeline_20210423090034.json\"   # Null pipeline type \n  df_pipeline = spark.read.option(\"multiLine\",\"true\").json(pipeline_filepath)\n  #set varivable for Audit Control \n  pipeline_min_records = df_pipeline.agg({\"TotalRows\" : \"min\"}).collect()[0][0]\n  pipeline_max_records = df_pipeline.agg({\"TotalRows\" : \"max\"}).collect()[0][0]\n  \n  df_pipeline = df_pipeline.withColumn(\"data\",explode(col('data'))).select(\"data.*\")\n\n  df_pipeline = df_pipeline.select(\"Snapshot Date[Snapshot Date]\", \"Snapshot Date[Relative Snapshot Month Offset]\", \"Date[End of Month]\", \"Date[Relative Month Offset]\",\\\n                                   \"Contributing Studio Location[Contrib Sub Region Code]\", \"Pipeline Type[Pipeline Type]\", \"[Pipeline Value]\",\"[Pipeline at 100 Percent]\",\"[Yield]\")\n\n  df_pipeline = df_pipeline.withColumnRenamed(\"Snapshot Date[Snapshot Date]\",\"Snapshot_Date\")\\\n                           .withColumnRenamed(\"Snapshot Date[Relative Snapshot Month Offset]\",\"Relative Snapshot Month Offset\")\\\n                           .withColumnRenamed(\"Date[End of Month]\",\"End_of_Month\")\\\n                           .withColumnRenamed(\"Date[Relative Month Offset]\",\"Relative Month Offset\")\\\n                           .withColumnRenamed(\"Contributing Studio Location[Contrib Sub Region Code]\",\"Contrib Sub Region Code\")\\\n                           .withColumnRenamed(\"Pipeline Type[Pipeline Type]\",\"Pipeline Type\")\\\n                           .withColumnRenamed(\"[Pipeline Value]\",\"Pipeline\")\\\n                           .withColumnRenamed(\"[Pipeline at 100 Percent]\",\"Pipeline at 100 Percent\")\\\n                           .withColumnRenamed(\"[Yield]\",\"Yield\")\n\n  df_pipeline = df_pipeline.withColumn(\"Snapshot_Date\",  df_pipeline[\"Snapshot_Date\"].cast(DateType()))\n  df_pipeline = df_pipeline.withColumn(\"Relative Snapshot Month Offset\",  df_pipeline[\"Relative Snapshot Month Offset\"].cast(IntegerType()))\n  df_pipeline = df_pipeline.withColumn(\"End_of_Month\",  df_pipeline[\"End_of_Month\"].cast(DateType()))\n  df_pipeline = df_pipeline.withColumn(\"Relative Month Offset\",  df_pipeline[\"Relative Month Offset\"].cast(IntegerType()))\n  df_pipeline = df_pipeline.withColumn(\"Contrib Sub Region Code\",  df_pipeline[\"Contrib Sub Region Code\"].cast(StringType()))\n\n  df_pipeline = df_pipeline.withColumn(\"Pipeline\",regexp_replace(\"Pipeline\", \",\", ''))\n  df_pipeline = df_pipeline.withColumn(\"Pipeline\",  df_pipeline[\"Pipeline\"].cast(DecimalType(32,6)))\n\n  df_pipeline = df_pipeline.withColumn(\"Pipeline at 100 Percent\",regexp_replace(\"Pipeline at 100 Percent\", \",\", ''))\n  df_pipeline = df_pipeline.withColumn(\"Pipeline at 100 Percent\",  df_pipeline[\"Pipeline at 100 Percent\"].cast(DecimalType(32,6)))\n\n  df_pipeline = df_pipeline.withColumn(\"Yield\",regexp_replace(\"Yield\", \"%\", ''))\n  df_pipeline = df_pipeline.withColumn(\"Yield\",  df_pipeline[\"Yield\"].cast(DecimalType(8,4)) * lit(0.01))\n  \n  #set varivable for Audit Control \n  pipeline_record_count = df_pipeline.agg({\"*\" : \"count\"}).collect()[0][0]\n  pipeline_type_null_count = df_pipeline.filter(col(\"Pipeline Type\").isNull()).agg({\"Snapshot_Date\" : \"count\"}).collect()[0][0]\n  #Audit Control \n  check_audit_contorls(\"pipeline\",pipeline_record_count,pipeline_min_records,pipeline_max_records,pipeline_type_null_count)\n\n  #pipeine trend\n  #pipeline_trend_filepath = \"/mnt/dev/raw/enterprise/bimodelapi/summary/pipeline trend/2021/05/05/pipeline trend_20210505131204.json\"\n  df_pipeline_trend = spark.read.option(\"multiLine\",\"true\").json(pipeline_trend_filepath)\n  #set varivable for Audit Control \n  pipeline_trend_min_records = df_pipeline_trend.agg({\"TotalRows\" : \"min\"}).collect()[0][0]\n  pipeline_trend_max_records = df_pipeline_trend.agg({\"TotalRows\" : \"max\"}).collect()[0][0]\n  \n  df_pipeline_trend = df_pipeline_trend.withColumn(\"data\",explode(col('data'))).select(\"data.*\")\n  df_pipeline_trend = df_pipeline_trend.select(\"Snapshot Date[Snapshot Date]\", \"Snapshot Date[Snapshot End of Month]\",\\\n                                   \"Contributing Studio Location[Contrib Sub Region Code]\", \"[Pipeline Trend Value]\")\n\n  df_pipeline_trend = df_pipeline_trend.withColumnRenamed(\"Snapshot Date[Snapshot Date]\",\"Snapshot_Date\")\\\n                                       .withColumnRenamed(\"Snapshot Date[Snapshot End of Month]\",\"End_of_Month\")\\\n                                       .withColumnRenamed(\"Contributing Studio Location[Contrib Sub Region Code]\",\"Contrib Sub Region Code\")\\\n                                       .withColumnRenamed(\"[Pipeline Trend Value]\",\"Pipeline Trend\")\n\n  df_pipeline_trend = df_pipeline_trend.withColumn(\"Snapshot_Date\",  df_pipeline_trend[\"Snapshot_Date\"].cast(DateType()))\n  df_pipeline_trend = df_pipeline_trend.withColumn(\"End_of_Month\",  df_pipeline_trend[\"End_of_Month\"].cast(DateType()))\n  df_pipeline_trend = df_pipeline_trend.withColumn(\"Contrib Sub Region Code\",  df_pipeline_trend[\"Contrib Sub Region Code\"].cast(StringType()))\n  df_pipeline_trend = df_pipeline_trend.withColumn(\"Pipeline Trend\",regexp_replace(\"Pipeline Trend\", \",\", ''))\n  df_pipeline_trend = df_pipeline_trend.withColumn(\"Pipeline Trend\",  df_pipeline_trend[\"Pipeline Trend\"].cast(DecimalType(32,6)))\n\n  #set varivable for Audit Control \n  pipeline_trend_record_count = df_pipeline_trend.agg({\"*\" : \"count\"}).collect()[0][0]\n  #Audit Control \n  check_audit_contorls(\"pipeline_trend\",pipeline_trend_record_count,pipeline_trend_min_records,pipeline_trend_max_records)\n\n#   #Convert pyspark dataframe to pandas dataframe.\n#   revhist = df_gl.toPandas()\n#   opphist = df_opportunities.toPandas()\n#   projhist = df_projects.toPandas()\n#   talenthist = df_headcount.toPandas()\n#   projectactualshist = df_project_allocations.toPandas()\n#   pipehist = df_pipeline.toPandas()\n#   pipetrend = df_pipeline_trend.toPandas()\n\n  target_file_name = \"OppHistory.csv\"\n  generate_automl_input_files(df_opportunities, environment, storage_name, target_file_name)\n  \n  target_file_name = \"PipelineHistory.csv\"\n  generate_automl_input_files(df_pipeline, environment, storage_name, target_file_name)\n  \n  target_file_name = \"PipelineTrend.csv\"\n  generate_automl_input_files(df_pipeline_trend, environment, storage_name, target_file_name)\n\n  target_file_name = \"ProjectActualsHistory.csv\"\n  generate_automl_input_files(df_project_allocations, environment, storage_name, target_file_name)\n\n  target_file_name = \"ProjectHistory.csv\"\n  generate_automl_input_files(df_projects, environment, storage_name, target_file_name)\n\n  target_file_name = \"RevenueHistory.csv\"\n  generate_automl_input_files(df_gl, environment, storage_name, target_file_name)  \n  \n  target_file_name = \"TalentHistory.csv\"\n  generate_automl_input_files(df_headcount, environment, storage_name, target_file_name)\n  \n  \nexcept Exception as error:\n  print(error)\n  log_error(\"{} {}\".format(notebook, error)) #log error in sentry\n  #raise dbutils.notebook.exit(error) #raise the exception\n  raise error #raise the exception"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Converting to pandas dataframe","showTitle":true,"inputWidgets":{},"nuid":"aa8e4d10-a18f-4485-89d5-8cc6c7f4a8bb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df_gl","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"End_of_Month","nullable":true,"type":"date"},{"metadata":{},"name":"Managing Sub Region Code","nullable":true,"type":"string"},{"metadata":{},"name":"Revenue","nullable":true,"type":"decimal(32,6)"}],"type":"struct"},"tableIdentifier":null},{"name":"df_opportunities","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"End_of_Month","nullable":true,"type":"date"},{"metadata":{},"name":"Managing Sub Region Code","nullable":true,"type":"string"},{"metadata":{},"name":"Opportunity Period Count","nullable":true,"type":"integer"},{"metadata":{},"name":"Opportunity Count","nullable":true,"type":"integer"},{"metadata":{},"name":"Current Opp Period Value","nullable":true,"type":"decimal(32,6)"},{"metadata":{},"name":"Opportunity Value","nullable":true,"type":"decimal(32,6)"},{"metadata":{},"name":"Win Rate","nullable":true,"type":"double"},{"metadata":{},"name":"Current Opp Period Count","nullable":true,"type":"integer"},{"metadata":{},"name":"Opportunity Period Value","nullable":true,"type":"decimal(32,6)"}],"type":"struct"},"tableIdentifier":null},{"name":"df_projects","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"End_of_Month","nullable":true,"type":"date"},{"metadata":{},"name":"Managing Sub Region Code","nullable":true,"type":"string"},{"metadata":{},"name":"Project Period Count","nullable":true,"type":"integer"},{"metadata":{},"name":"Project Count","nullable":true,"type":"integer"},{"metadata":{},"name":"Project Period Price","nullable":true,"type":"decimal(32,6)"},{"metadata":{},"name":"Project Price","nullable":true,"type":"decimal(32,6)"},{"metadata":{},"name":"Conversions","nullable":true,"type":"decimal(32,6)"}],"type":"struct"},"tableIdentifier":null},{"name":"df_headcount","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"End_of_Month","nullable":true,"type":"date"},{"metadata":{},"name":"Contrib Sub Region Code","nullable":true,"type":"string"},{"metadata":{},"name":"Journey Level","nullable":true,"type":"string"},{"metadata":{},"name":"Billable","nullable":true,"type":"string"},{"metadata":{},"name":"Headcount","nullable":true,"type":"decimal(32,6)"},{"metadata":{},"name":"Headcount Contingent","nullable":true,"type":"decimal(32,6)"}],"type":"struct"},"tableIdentifier":null},{"name":"df_project_allocations","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"End_of_Month","nullable":true,"type":"date"},{"metadata":{},"name":"Contrib Sub Region Code","nullable":true,"type":"string"},{"metadata":{},"name":"Journey Level","nullable":true,"type":"string"},{"metadata":{},"name":"Utilization Billable","nullable":true,"type":"double"},{"metadata":{},"name":"Nominal_Hours","nullable":true,"type":"decimal(32,6)"}],"type":"struct"},"tableIdentifier":null},{"name":"df_pipeline","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"Snapshot_Date","nullable":true,"type":"date"},{"metadata":{},"name":"Relative Snapshot Month Offset","nullable":true,"type":"integer"},{"metadata":{},"name":"End_of_Month","nullable":true,"type":"date"},{"metadata":{},"name":"Relative Month Offset","nullable":true,"type":"integer"},{"metadata":{},"name":"Contrib Sub Region Code","nullable":true,"type":"string"},{"metadata":{},"name":"Pipeline Type","nullable":true,"type":"string"},{"metadata":{},"name":"Pipeline","nullable":true,"type":"decimal(32,6)"},{"metadata":{},"name":"Pipeline at 100 Percent","nullable":true,"type":"decimal(32,6)"},{"metadata":{},"name":"Yield","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null},{"name":"df_pipeline_trend","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"Snapshot_Date","nullable":true,"type":"date"},{"metadata":{},"name":"End_of_Month","nullable":true,"type":"date"},{"metadata":{},"name":"Contrib Sub Region Code","nullable":true,"type":"string"},{"metadata":{},"name":"Pipeline Trend","nullable":true,"type":"decimal(32,6)"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">Converting to pandas...\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Converting to pandas...\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3bdd3119-2b5e-4054-a7f1-3433955b3eb6"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"dl_pbiapi_summary","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{"data_load_type":{"nuid":"6846f23c-3f3e-4fb9-b799-f3fdcbff42a0","currentValue":"Full","widgetInfo":{"widgetType":"text","name":"data_load_type","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"system_name":{"nuid":"4dd64b70-a402-4d77-af5e-2e6385ad0454","currentValue":"bimodelapi","widgetInfo":{"widgetType":"text","name":"system_name","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}},"environment":{"nuid":"5cf3993e-e32b-4706-94ee-eed176022bcb","currentValue":"prod","widgetInfo":{"widgetType":"text","name":"environment","defaultValue":"","label":"","options":{"widgetType":"text","validationRegex":null}}}},"notebookOrigID":4405127232634668}},"nbformat":4,"nbformat_minor":0}
