{"cells":[{"cell_type":"code","source":["from datetime import date, timedelta, datetime\nimport hashlib \nimport stringcase\nimport pandas as pd\nfrom pyspark.sql.functions import col,concat,lit,current_date, lag, lead, first, last, desc, hash, date_format,coalesce\nfrom pyspark.sql import *\nfrom delta.tables import *\nfrom pyspark.sql.types import TimestampType, LongType, StringType\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\nfrom traitlets.config.loader import Config\n\ndef encrypt_value(encrvalue):\n  sha_value = hashlib.sha256(encrvalue.encode()).hexdigest()\n  #sha_value_int = int(sha_value, base=16)\n  return sha_value\n\ngenerate_hash = udf(encrypt_value)\n\ndef getNotebookUser():\n  return dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\n\ndef getNotebookPath():\n  return dbutils.notebook.entry_point.getDbutils().notebook().getContext().extraContext().apply('notebook_path')\n\ndef getUtcTimeNow():\n  return datetime.strftime(datetime.utcnow(),'%Y-%m-%d %H:%M:%SZ')\n\ndef push_to_csv(environment, storage_name, folder_path, file_name):    \n    \n    output_container_name = environment\n    \n    # setting output paths\n    output_container_path = \"wasbs://{}@{}.blob.core.windows.net\".format(output_container_name, storage_name)\n    output_blob_folder = \"{}/curated/temp/{}\".format(output_container_path, file_name.replace(\".csv\", \"\"))\n    \n    output_blob_target = \"{}/{}/{}\".format(output_container_path, folder_path, file_name)\n    # list files\n    files = dbutils.fs.ls(output_blob_folder)\n\n    # find data file\n    output_file = [x for x in files if x.name.startswith(\"part-00000\")]\n\n    # copy data file under correct folder with appropriate name\n    dbutils.fs.cp(output_file[0].path, output_blob_target)\n\n    # remove temp fodler\n    dbutils.fs.rm(output_blob_folder, True)\n\ndef push_to_sql(environment,df,table_name):\n  Hostname = None\n  Database = None\n  Port = None\n  UN = None\n  PW = None\n   \n  if(environment == \"dev\"):\n    Hostname = \"d-win-sql-azwu-01.database.windows.net\"\n    Database = \"sqldb-bimodel-devqa-01\"\n    Port = 1433\n    UN = 'sqladmin'\n    PW = 'serveradmin@123'\n\n  if(environment == \"prod\"):\n    Hostname = \"d-win-sql-azwu-01.database.windows.net\"\n    Database = \"sqldb-bimodel-devqa-01\"\n    Port = 1433\n    UN = 'sqladmin'\n    PW = 'serveradmin@123'\n    \n  Url = \"jdbc:sqlserver://{0}:{1};database={2};user={3};password= {4}\".format(Hostname, Port, Database, UN, PW)\n  sql_table_name = \"[\"+table_name.replace(\".csv\",\"\")+\"]\"\n  #print(\"push_to_sql\")\n  #display(df) \n  df.write.jdbc(Url, sql_table_name, mode=\"overwrite\")\n  print(sql_table_name)\n  \ndef process_curated(environment,system_name,table_name = \"\"):\n\n  #initializing variables\n  g_bi_config_parameters_path = \"/mnt/\"+ environment + \"/gold/g_bi_config_parameters\"\n  g_bi_config_curated_tables_columns_path = \"/mnt/\"+ environment + \"/gold/g_bi_config_curated_tables_columns\"\n  \n  #reading config tables\n  df_bi_configuration = spark.read.format(\"delta\").load(g_bi_config_parameters_path)\n  df_bi_configuration = df_bi_configuration.filter(df_bi_configuration.SystemName == lit(system_name))\n  df_bi_config_curated_tables_columns  = spark.read.format(\"delta\").load(g_bi_config_curated_tables_columns_path)\n  \n  #initializing config parameter values\n  curated_folder_path   =  df_bi_configuration.filter(df_bi_configuration.ParameterName == \"curated_folder_path\")\\\n                                                     .select(\"ParameterValue\")\\\n                                                     .collect()[0][0]\n\n  platinum_folder_path  =  df_bi_configuration.filter(df_bi_configuration.ParameterName == \"platinum_folder_path\")\\\n                                                      .select(\"ParameterValue\")\\\n                                                      .collect()[0][0]\n\n  #initialize storage account \n  storage_name = df_bi_configuration.filter(df_bi_configuration.ParameterName == \"storage_name\")\\\n                                                 .select(\"ParameterValue\")\\\n                                                 .collect()[0][0]\n\n  storage_key = None\n  if(environment == \"dev\"):\n    storage_key  = dbutils.secrets.get(scope=\"kv-bi-devqa-01-secrets\", key=\"databricks-{}-storage-key\".format(environment))\n    \n  if(environment == \"prod\"):\n    storage_key  = dbutils.secrets.get(scope=\"kv-bi-devqa-01-secrets\", key=\"databricks-{}-storage-key\".format(environment))\n \n  #initializing config parameter values\n  spark.conf.set(\"fs.azure.account.key.{}.blob.core.windows.net\".format(storage_name),storage_key)\n  \n  if len(table_name) > 0:\n      df_bi_config_curated_tables  = df_bi_config_curated_tables_columns.filter(df_bi_config_curated_tables_columns.SourceTableName == lit(table_name))\n  else:\n      df_bi_config_curated_tables  = df_bi_config_curated_tables_columns.filter(df_bi_config_curated_tables_columns.SystemName == lit(system_name))\n  \n  table_rows  = df_bi_config_curated_tables.select(\"SourceTableName\",\"TargetFilePath\",\"TargetFileName\").distinct().collect()\n    \n  def getTable(i):\n    nonlocal platinum_folder_path\n    nonlocal curated_folder_path\n    nonlocal df_bi_config_curated_tables_columns\n    nonlocal environment   # TS - for csv output \n    nonlocal storage_name  # TS - for csv output \n    \n    source_table_name = i[0]\n    target_file_path = i[1]\n    target_file_name = i[2]\n\n    p_table_name_path = platinum_folder_path + \"/{}\".format(source_table_name) \n    curated_folder_path_temp = curated_folder_path + \"/temp/{}\".format(target_file_name.replace(\".csv\",\"\"))\n    \n    df_bi_config_curated_tables_columns_final  = df_bi_config_curated_tables_columns\\\n                                                 .filter(df_bi_config_curated_tables_columns.SourceTableName == lit(source_table_name))\n    \n    #initialize platinum dataframe\n    p_df_table_name = spark.read.format(\"delta\").load(p_table_name_path)\n    \n    #only export valid record in CSV file\n    #p_df_table_name = p_df_table_name \\\n    #                  .where(p_df_table_name.ValidFromDateKey <= p_df_table_name.ValidToDateKey)\n\n    p_table_name_schema = p_df_table_name.select(\"*\").schema\n    p_df_table_name = p_df_table_name.select(p_table_name_schema.fieldNames())\n    \n    for col in p_df_table_name.columns:        \n      try:\n        new_col = df_bi_config_curated_tables_columns_final\\\n                  .filter(df_bi_config_curated_tables_columns_final.SourceFieldName == lit(col)).select(\"TargetFieldName\").collect()[0][0]\n        p_df_table_name = p_df_table_name.withColumnRenamed(col,new_col)\n      except IndexError:\n            print(source_table_name + \"-> Column Name: \" + col)\n            break\n      \n    if (system_name == \"bimodelapi\"):  # TS - for csv output added if condition\n      p_df_table_name\\\n       .coalesce(1)\\\n       .write\\\n       .mode(\"overwrite\")\\\n       .option(\"header\", \"true\")\\\n       .format(\"com.databricks.spark.csv\")\\\n       .save(curated_folder_path_temp)\n      \n      push_to_csv(environment, storage_name, target_file_path, target_file_name)\n      \n    #writing dataframe to Azure sql server\n    push_to_sql(environment,p_df_table_name,target_file_name)\n   \n\n  with ThreadPoolExecutor(max_workers=20) as pool:\n    result = pool.map(getTable,table_rows)\n\n#   writing dataframe to temp  folder\n#   p_df_table_name\\\n#      .coalesce(1)\\\n#      .write\\\n#      .mode(\"overwrite\")\\\n#      .option(\"header\", \"true\")\\\n#      .format(\"com.databricks.spark.csv\")\\\n#      .save(curated_folder_path_temp)\n\n#   push_to_csv(environment, storage_name, target_file_path, target_file_name)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1dca0bb2-881c-4971-adcb-fd63214359ff"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# ===================================== Common Helper Functions ==================================== #\nimport hashlib\nimport os\n\n# save file as json.\ndef write_to_json(filepath, data):\n  try:\n    dbutils.fs.rm(filepath) #if file alreay exist then delete it first\n    dbutils.fs.put(filepath, json.dumps(data))\n    return filepath\n  except Exception as error:\n    log_error(\"{} {}\".format(notebook, error), filepath)\n\n# save file as csv.\ndef _write_to_csv(filepath, data):\n  return \"waiting for csv file\"\n\n# hash code generator.\ndef encrypt_value(encrvalue):\n  sha_value = hashlib.sha256(encrvalue.encode()).hexdigest()\n  return sha_value\ngenerate_hash = udf(encrypt_value)\n\n# return bronze path\ndef get_bronze_path(tablename):\n  return \"/mnt/{}/bronze/{}\".format(environment, tablename)\n\n# return dataload audit fields\ndef get_dl_audit_fields(dataframe):\n  dataframe = dataframe.withColumn(\"IBIBatchID\", lit(batch_id).cast(LongType()))\n  dataframe = dataframe.withColumn(\"IBIBatchStartDateTime\", lit(datetime.utcnow()))\n  dataframe = dataframe.withColumn(\"dl_updated_datetime\", lit(datetime.utcnow()))\n  return dataframe\n\n# get file path\ndef get_filepath(source_path, system, entity, batch_id):\n  raw_path =  \"{}/{}/{}/{}/{}/{}\".format(source_path, \n                                               system,\n                                               entity,\n                                               batch_start_datetime.strftime(\"%Y\"), \n                                               batch_start_datetime.strftime(\"%m\"),\n                                               batch_start_datetime.strftime(\"%d\"))\n  return \"{}/{}_{}.json\".format(raw_path, entity, batch_id)\n\ndef convert_str_to_timestamp(date_col):\n    if(date_col is None):\n      date_col=\"01/01/1900\"\n    _date = datetime.strptime(date_col, '%m/%d/%Y')\n    return _date.strftime('%Y-%m-%d 00:00:00')\n\ntimestamp_udf = udf(convert_str_to_timestamp)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6575ba1b-f245-46d5-aa67-e7eb88d1ef46"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def generate_automl_input_files(df, environment, storage_name, target_file_name):\n    target_file_path = \"/automl_rev_region_forecast/inputs/\"  \n    curated_folder_path_temp = curated_folder_path + \"/temp/{}\".format(target_file_name.replace(\".csv\",\"\"))\n    #display(df)\n    df\\\n       .coalesce(1)\\\n       .write\\\n       .mode(\"overwrite\")\\\n       .option(\"header\", \"true\")\\\n       .format(\"com.databricks.spark.csv\")\\\n       .save(curated_folder_path_temp)\n\n    push_to_csv(environment, storage_name, target_file_path, target_file_name)\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b45460c9-b65c-45fb-9778-1a4495113428"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def check_audit_contorls(api_name,record_count,min_records,max_records,pipeline_type_null_count = 0):\n  #Audit Control \n  error_msg = api_name\n  if (min_records != max_records):\n    error_msg = error_msg + \" Min/Max record not matched {}/{} ....\".format(str(min_records),str(max_records))\n    \n  if (api_name == \"pipeline\"):\n    if pipeline_type_null_count > 0 :\n      error_msg = error_msg + \" Null Pipeline Type found in API data....\"\n  \n  if (record_count != max_records):\n    error_msg = error_msg  + \" Rrecord count not matched with total records {}/{}....\".format(str(record_count),str(max_records))\n\n  if (error_msg != api_name):\n    raise(Exception(\"Data Error\", error_msg))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11d699f4-bd32-45af-b3a2-ad9fbcf82db6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"lib_helper","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2892597332672951}},"nbformat":4,"nbformat_minor":0}
