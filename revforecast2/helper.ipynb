{"cells":[{"cell_type":"code","source":["#!/usr/bin/python\n","# -*- coding: utf-8 -*-\n","# Definitions\n","\n","import pandas as pd\n","import numpy as np\n","\n","\n","# entity_debug = \"GBR\"\n","\n","debug = False\n","\n","# display count and summary of any dataframe\n","\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.expand_frame_repr', False)\n","#pd.set_option('max_colwidth', -1)\n","pd.set_option('display.precision', 1)\n","pd.set_option('display.float_format', lambda x: '%.2f' % x)\n","\n","\n","def get_json(df):\n","    \"\"\" Small function to serialise DataFrame dates as 'YYYY-MM-DD' in JSON \"\"\"\n","\n","    def convert_timestamp(item_date_object):\n","        if isinstance(item_date_object, (datetime.date,\n","                      datetime.datetime)):\n","            return item_date_object.strftime('%Y-%m-%d')\n","\n","    dict_ = df.to_dict(orient='records')\n","\n","    return json.dumps(dict_, default=convert_timestamp)\n","\n","\n","def get_df_name(df):\n","    name = [x for x in globals() if globals()[x] is df][0]\n","    return name\n","\n","\n","def difflist(li1, li2):\n","    return list(set(li1) - set(li2))\n","\n","\n","def addlist(li1, li2):\n","    return li1.append(li2)\n","\n","\n","def remove_percetage(df, column_list):\n","    for col in column_list:\n","        df[col] = round(df[col].str.replace('%', '').astype(np.float64)\n","                        / 100, 4)\n","    return df\n","\n","\n","def coerce_df_columns_to_numeric(df):\n","    df = df.infer_objects()\n","    cols_float1 = list(df.filter(like='Rate', axis=1).columns)\n","    cols_float2 = list(df.filter(like='Yield', axis=1).columns)\n","    cols_float3 = list(df.filter(like='Diff%', axis=1).columns)\n","    cols_float4 = list(df.filter(like='Relative_Offset',\n","                       axis=1).columns)\n","    cols_float5 = list(df.filter(like='sp500', axis=1).columns)\n","    cols_float6 = list(df.filter(like='Return', axis=1).columns)\n","    cols_float = cols_float1 + cols_float2 + cols_float3 + cols_float4 \\\n","        + cols_float5 + cols_float6\n","\n","    # display(\"cols_float:\", cols_float)\n","\n","    cols_int1 = list(df.filter(like='Revenue', axis=1).columns)\n","    cols_int2 = list(df.filter(like='Conversions', axis=1).columns)\n","    cols_int3 = list(df.filter(like='Value', axis=1).columns)\n","    cols_int4 = list(df.filter(like='Pipeline', axis=1).columns)\n","    cols_int5 = list(df.filter(like='Offset', axis=1).columns)\n","    cols_int6 = list(df.filter(like='Headcount', axis=1).columns)\n","    cols_int = cols_int1 + cols_int2 + cols_int3 + cols_int4 \\\n","        + cols_int5 + cols_int6\n","\n","    # display(\"cols_int:\", cols_int)\n","\n","    cols1 = list(df.select_dtypes(include='float64').columns)\n","    cols = cols1 + cols_int\n","\n","    # display(\"cols:\", cols)\n","\n","    final_cols = difflist(cols, cols_float)\n","\n","    # display(\"final_cols:\", final_cols)\n","\n","    df[final_cols] = df[final_cols].apply(pd.to_numeric, errors='coerce'\n","            )\n","    df[final_cols] = df[final_cols].replace(np.nan, 0, regex=True)\n","    df[final_cols] = df[final_cols].astype(int)\n","    df[final_cols] = round(df[final_cols], 0)\n","\n","\n","    # return df\n","    \n","def convert_date_cols(df):\n","  if 'End_of_Month' in df.columns:\n","        df['End_of_Month'] = pd.to_datetime(df['End_of_Month'])\n","  if 'Snapshot_Date_Short' in df.columns:\n","        df['Snapshot_Date_Short'] = \\\n","            pd.to_datetime(df['Snapshot_Date_Short']) \n","  if 'Forecast_Date' in df.columns:\n","        df['Forecast_Date'] = \\\n","            pd.to_datetime(df['Forecast_Date']) \n","  return df\n","\n","def data_prep(df):\n","    df.columns = df.columns.astype(str).str.replace(' ', '_')\n","    df = convert_date_cols(df)\n","    df = df.replace(np.nan, 0, regex=True)\n","    return df\n","\n","\n","def show_stats(df):\n","    print (' DF Name: ')\n","    display(get_df_name(df))\n","    print (' DF Info: ')\n","    display(df.info(verbose=True))\n","    print (' DF Describe: ')\n","    display(df.describe(include='all').transpose().head())\n","    print (' DF Head: ')\n","    display(df.head())\n","    print (' DF Tail: ')\n","    display(df.tail())\n","\n","    # group_by_entity = df.groupby(by=['Fin_Entity_ID'], as_index=False)\n","    # entity_sum = group_by_entity.sum().reset_index(drop=True)\n","    # entity_count = group_by_entity.count().reset_index(drop=True)\n","    # print(\" Entity Sum: \")\n","    # display(entity_sum.head())\n","    # print(\" Studio Count: \")\n","    # display(entity_count.head())\n","\n","    if 'End_of_Month' in df.columns:\n","        df['End_of_Month'] = pd.to_datetime(df['End_of_Month'])  # Format Date\n","        group_by_eom = df.groupby(by=['End_of_Month'], as_index=False)\n","        eom_sum = group_by_eom.sum().reset_index(drop=True)\n","        eom_count = group_by_eom.count().reset_index(drop=True)\n","        print (' EOM Sum:')\n","        display(eom_sum.head())\n","        print (' EOM Count: ')\n","        display(eom_count.head())\n","    if 'Snapshot_Date_Short' in df.columns:\n","        df['Snapshot_Date_Short'] = \\\n","            pd.to_datetime(df['Snapshot_Date_Short'])  # Format Date\n","        group_by_sds = df.groupby(by=['Snapshot_Date_Short'],\n","                                  as_index=False)\n","        sds_sum = group_by_sds.sum().reset_index(drop=True)\n","        sds_count = group_by_sds.count().reset_index(drop=True)\n","        print (' SDS Sum:')\n","        display(sds_sum.head())\n","        print (' SDS Count: ')\n","        display(sds_count.head())\n","    return\n","  \n","def split_last_n_by_series_id(df, n):\n","    \"\"\"Group df by series identifiers and split on last n rows for each group.\"\"\"\n","\n","    df_grouped = \\\n","        df.sort_values(time_column_name).groupby(time_series_id_column_names,\n","            group_keys=False)  # Sort by ascending time\n","    df_head = df_grouped.apply(lambda dfg: dfg.iloc[:-n])\n","    df_tail = df_grouped.apply(lambda dfg: dfg.iloc[-n:])\n","    return (df_head, df_tail)\n","\n","def APE(actual, pred):\n","    \"\"\"\n","    Calculate absolute percentage error.\n","    Returns a vector of APE values with same length as actual/pred.\n","    \"\"\"\n","\n","    return 100 * np.abs((actual - pred) / actual)\n","\n","\n","def MAPE(actual, pred):\n","    \"\"\"\n","    Calculate mean absolute percentage error.\n","    Remove NA and values where actual is close to zero\n","    \"\"\"\n","\n","    not_na = ~(np.isnan(actual) | np.isnan(pred))\n","    not_zero = ~np.isclose(actual, 0.0)\n","    actual_safe = actual[not_na & not_zero]\n","    pred_safe = pred[not_na & not_zero]\n","    return np.mean(APE(actual_safe, pred_safe))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e4aa933-d9b5-414c-9266-780bc99ae55b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#!/usr/bin/python\n","# -*- coding: utf-8 -*-\n","import pandas as pd\n","import numpy as np\n","from pandas.tseries.frequencies import to_offset\n","\n","\n","def align_outputs(\n","    y_predicted,\n","    X_trans,\n","    X_test,\n","    y_test,\n","    target_column_name,\n","    predicted_column_name='predicted',\n","    horizon_colname='horizon_origin',\n","    ):\n","    \"\"\"\n","    Demonstrates how to get the output aligned to the inputs\n","    using pandas indexes. Helps understand what happened if\n","    the output's shape differs from the input shape, or if\n","    the data got re-sorted by time and grain during forecasting.\n","\n","    Typical causes of misalignment are:\n","    * we predicted some periods that were missing in actuals -> drop from eval\n","    * model was asked to predict past max_horizon -> increase max horizon\n","    * data at start of X_test was needed for lags -> provide previous periods\n","    \"\"\"\n","\n","    if horizon_colname in X_trans:\n","        df_fcst = pd.DataFrame({predicted_column_name: y_predicted,\n","                               horizon_colname: X_trans[horizon_colname]})\n","    else:\n","        df_fcst = pd.DataFrame({predicted_column_name: y_predicted})\n","\n","    # y and X outputs are aligned by forecast() function contract\n","\n","    df_fcst.index = X_trans.index\n","\n","    # align original X_test to y_test\n","\n","    X_test_full = X_test.copy()\n","    X_test_full[target_column_name] = y_test\n","\n","    # X_test_full's index does not include origin, so reset for merge\n","\n","    df_fcst.reset_index(inplace=True)\n","    X_test_full = X_test_full.reset_index().drop(columns='index')\n","    together = df_fcst.merge(X_test_full, how='right')\n","\n","    # drop rows where prediction or actuals are nan\n","    # happens because of missing actuals\n","    # or at edges of time due to lags/rolling windows\n","\n","    clean = together[together[[target_column_name,\n","                     predicted_column_name]].notnull().all(axis=1)]\n","    return clean\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2a0ee83-41e3-4aff-8e7a-4d40534fcbe5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":[],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c2f3f09-10d3-41b8-b21c-17ddad269261"}},"outputs":[],"execution_count":0}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.9","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3.6 - AzureML","language":"python","name":"python3-azureml"},"kernel_info":{"name":"python3-azureml"},"application/vnd.databricks.v1+notebook":{"notebookName":"helper","dashboards":[],"language":"python","widgets":{},"notebookOrigID":4496546398282648},"nteract":{"version":"nteract-front-end@1.0.0"}},"nbformat":4,"nbformat_minor":0}